{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/ml4h/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/anaconda/envs/ml4h/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/anaconda/envs/ml4h/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/anaconda/envs/ml4h/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/anaconda/envs/ml4h/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/anaconda/envs/ml4h/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import importlib\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, log_loss, average_precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertConfig, BertModel, InputExample\n",
    "from run_classifier_dataset_utils import convert_examples_to_features, parallel_convert_examples_to_features\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils import data\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe9c2143270>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Run the `create_data.py` script to generate the following files:\n",
    "\n",
    "- `cohort.csv`:\n",
    "  - Contains one record for each adult patient’s first ICU stay over 48 hours in lengthwithin their first hospital admission.\n",
    "  - The `mort_icu` column represents whether the patient died during their ICU stay.\n",
    "  - The columns from `Acute Renal` to `Shock` correspond to each of the 25 CCS code  groups, which are derived from  ICD-9 codes assigned at the end of a patient’s hospital stay.\n",
    "  - The `Any Acute` and `Any Chronic` columns are derived from whether the patient has any acute and chronic phenotypes respectively.\n",
    "  \n",
    "  \n",
    "- `notes.csv`\n",
    "  - Contains, for each of the patients in `cohort.csv`, all of the notes written during their hospital stay (along with the timestamp) for the following note types:\n",
    "      - Discharge Summary\n",
    "      - Nursing\n",
    "      - Nursing/other\n",
    "  - The notes have been lightly preprocessed (ex: removing PHI identifiers, removing section numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'mimic_data/'\n",
    "bert_path = 'section2_pre/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/'\n",
    "# bert_path = '/scratch/gobi1/haoran/shared_data/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/'\n",
    "#/data/home/linda/ml4h_workspace/Problem_Set_2/section2_pre/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(bert_path).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = pd.read_hdf(os.path.join(data_path, 'cohort.h5'))\n",
    "notes = pd.read_hdf(os.path.join(data_path, 'notes.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>admittime</th>\n",
       "      <th>dischtime</th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>language</th>\n",
       "      <th>insurance</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>mort_icu</th>\n",
       "      <th>intime</th>\n",
       "      <th>Acute Renal</th>\n",
       "      <th>Cerebrovascular</th>\n",
       "      <th>Myocardial</th>\n",
       "      <th>Dysrhythmias</th>\n",
       "      <th>Chronic Kidney</th>\n",
       "      <th>COPD</th>\n",
       "      <th>Comp. Surgical</th>\n",
       "      <th>Conduction</th>\n",
       "      <th>Heart Failure</th>\n",
       "      <th>Atherosclerosis</th>\n",
       "      <th>Diabetes Comp</th>\n",
       "      <th>Diabetes No Comp</th>\n",
       "      <th>Lipid Metabolism</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Fluid Disorder</th>\n",
       "      <th>GI Hemorrhage</th>\n",
       "      <th>Hypertension Comp</th>\n",
       "      <th>Other Liver</th>\n",
       "      <th>Lower Resp</th>\n",
       "      <th>Upper Resp</th>\n",
       "      <th>Pleurisy</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Resp Failure</th>\n",
       "      <th>Septicemia</th>\n",
       "      <th>Shock</th>\n",
       "      <th>Any Acute</th>\n",
       "      <th>Any Chronic</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>211552</td>\n",
       "      <td>M</td>\n",
       "      <td>2101-10-20 19:08:00</td>\n",
       "      <td>2101-10-31 13:58:00</td>\n",
       "      <td>76.0</td>\n",
       "      <td>white</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2101-10-20 19:10:11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>107064</td>\n",
       "      <td>228232</td>\n",
       "      <td>F</td>\n",
       "      <td>2175-05-30 07:15:00</td>\n",
       "      <td>2175-06-15 16:00:00</td>\n",
       "      <td>65.0</td>\n",
       "      <td>white</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>English</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2175-05-30 21:30:54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>150750</td>\n",
       "      <td>220597</td>\n",
       "      <td>M</td>\n",
       "      <td>2149-11-09 13:06:00</td>\n",
       "      <td>2149-11-14 10:15:00</td>\n",
       "      <td>41.0</td>\n",
       "      <td>other</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Medicaid</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2149-11-09 13:07:02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>112213</td>\n",
       "      <td>232669</td>\n",
       "      <td>M</td>\n",
       "      <td>2104-08-07 10:15:00</td>\n",
       "      <td>2104-08-20 02:57:00</td>\n",
       "      <td>72.0</td>\n",
       "      <td>white</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2104-08-08 02:08:17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>143045</td>\n",
       "      <td>263738</td>\n",
       "      <td>F</td>\n",
       "      <td>2167-01-08 18:43:00</td>\n",
       "      <td>2167-01-15 15:15:00</td>\n",
       "      <td>39.0</td>\n",
       "      <td>white</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Medicaid</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2167-01-08 18:44:25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  hadm_id  icustay_id gender           admittime  \\\n",
       "0           3   145834      211552      M 2101-10-20 19:08:00   \n",
       "1           6   107064      228232      F 2175-05-30 07:15:00   \n",
       "2           9   150750      220597      M 2149-11-09 13:06:00   \n",
       "3          12   112213      232669      M 2104-08-07 10:15:00   \n",
       "4          13   143045      263738      F 2167-01-08 18:43:00   \n",
       "\n",
       "            dischtime   age ethnicity admission_type language insurance  \\\n",
       "0 2101-10-31 13:58:00  76.0     white      EMERGENCY  Missing  Medicare   \n",
       "1 2175-06-15 16:00:00  65.0     white       ELECTIVE  English  Medicare   \n",
       "2 2149-11-14 10:15:00  41.0     other      EMERGENCY  Missing  Medicaid   \n",
       "3 2104-08-20 02:57:00  72.0     white       ELECTIVE  Missing  Medicare   \n",
       "4 2167-01-15 15:15:00  39.0     white      EMERGENCY  Missing  Medicaid   \n",
       "\n",
       "   hospital_expire_flag  mort_icu              intime  Acute Renal  \\\n",
       "0                     0         0 2101-10-20 19:10:11          1.0   \n",
       "1                     0         0 2175-05-30 21:30:54          0.0   \n",
       "2                     1         1 2149-11-09 13:07:02          1.0   \n",
       "3                     1         0 2104-08-08 02:08:17          0.0   \n",
       "4                     0         0 2167-01-08 18:44:25          0.0   \n",
       "\n",
       "   Cerebrovascular  Myocardial  Dysrhythmias  Chronic Kidney  COPD  \\\n",
       "0              0.0         1.0           0.0             0.0   0.0   \n",
       "1              0.0         0.0           0.0             0.0   0.0   \n",
       "2              1.0         0.0           0.0             0.0   0.0   \n",
       "3              0.0         0.0           0.0             0.0   0.0   \n",
       "4              0.0         0.0           0.0             0.0   0.0   \n",
       "\n",
       "   Comp. Surgical  Conduction  Heart Failure  Atherosclerosis  Diabetes Comp  \\\n",
       "0             0.0         0.0            1.0              0.0            0.0   \n",
       "1             1.0         0.0            0.0              0.0            0.0   \n",
       "2             0.0         0.0            1.0              0.0            0.0   \n",
       "3             1.0         0.0            0.0              0.0            0.0   \n",
       "4             0.0         0.0            0.0              1.0            0.0   \n",
       "\n",
       "   Diabetes No Comp  Lipid Metabolism  Hypertension  Fluid Disorder  \\\n",
       "0               0.0               0.0           0.0             0.0   \n",
       "1               0.0               0.0           0.0             1.0   \n",
       "2               0.0               0.0           1.0             1.0   \n",
       "3               0.0               0.0           1.0             0.0   \n",
       "4               1.0               1.0           1.0             0.0   \n",
       "\n",
       "   GI Hemorrhage  Hypertension Comp  Other Liver  Lower Resp  Upper Resp  \\\n",
       "0            0.0                0.0          0.0         0.0         0.0   \n",
       "1            0.0                1.0          0.0         0.0         0.0   \n",
       "2            0.0                0.0          0.0         0.0         0.0   \n",
       "3            0.0                0.0          0.0         0.0         0.0   \n",
       "4            0.0                0.0          0.0         0.0         0.0   \n",
       "\n",
       "   Pleurisy  Pneumonia  Resp Failure  Septicemia  Shock  Any Acute  \\\n",
       "0       0.0        0.0           0.0         1.0    1.0        1.0   \n",
       "1       0.0        0.0           0.0         0.0    0.0        1.0   \n",
       "2       0.0        0.0           0.0         0.0    0.0        1.0   \n",
       "3       0.0        0.0           0.0         0.0    0.0        1.0   \n",
       "4       0.0        0.0           0.0         0.0    0.0        0.0   \n",
       "\n",
       "   Any Chronic  train  \n",
       "0          0.0      1  \n",
       "1          1.0      0  \n",
       "2          1.0      0  \n",
       "3          1.0      1  \n",
       "4          1.0      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19392, 42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>chartdate</th>\n",
       "      <th>charttime</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>22532</td>\n",
       "      <td>167853.0</td>\n",
       "      <td>2151-08-04</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>service: addendum:  radiologic s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>13702</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>2118-06-14</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>date of birth:                   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>26880</td>\n",
       "      <td>135453.0</td>\n",
       "      <td>2162-03-25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>date of birth:           ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>180</td>\n",
       "      <td>20646</td>\n",
       "      <td>134727.0</td>\n",
       "      <td>2112-12-10</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>service: medicine  aller...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>181</td>\n",
       "      <td>42130</td>\n",
       "      <td>114236.0</td>\n",
       "      <td>2150-03-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>date of birth:           ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   note_id  subject_id   hadm_id  chartdate charttime           category  \\\n",
       "0      174       22532  167853.0 2151-08-04       NaT  Discharge summary   \n",
       "1      175       13702  107527.0 2118-06-14       NaT  Discharge summary   \n",
       "4      178       26880  135453.0 2162-03-25       NaT  Discharge summary   \n",
       "6      180       20646  134727.0 2112-12-10       NaT  Discharge summary   \n",
       "7      181       42130  114236.0 2150-03-01       NaT  Discharge summary   \n",
       "\n",
       "                                                text  \n",
       "0                service: addendum:  radiologic s...  \n",
       "1               date of birth:                   ...  \n",
       "4                       date of birth:           ...  \n",
       "6                        service: medicine  aller...  \n",
       "7                       date of birth:           ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(425549, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_blank(text: str, model: BertForMaskedLM, tokenizer: BertTokenizer) -> (str, dict):\n",
    "    '''\n",
    "    Given a sentence with a single blank (denoted by an underscore), queries the BERT model to \n",
    "        fill in the missing token.\n",
    "        \n",
    "    Inputs:\n",
    "        - text: sentence containing a single underscore corresponding to the missing token\n",
    "                ex: \"[CLS] 40 yo asian homeless man with h/o polysubstance abuse and recently released from _  [SEP]\"\n",
    "        - model: pytorch ClinicalBERT model, of type BertForMaskedLM\n",
    "        - tokenizer: BertTokenizer object\n",
    "    \n",
    "    Output:\n",
    "        - tuple consisting of the following:\n",
    "            - string corresponding to the sentence where the underscore is replaced with the most likely token\n",
    "                ex: \"[CLS] 40 yo asian homeless man with h / o polysubstance abuse and recently released from home [SEP]\"\n",
    "            - a dictionary str:float mapping each word in the vocabulary to its normalized probability.\n",
    "                - sum of the values should be equal to 1\n",
    "                - the dictionary should have 28996 elements\n",
    "    '''\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # very important!!!\n",
    "    text = text.replace('_', \"[MASK]\")\n",
    "    \n",
    "    #https://stackoverflow.com/questions/54978443/predicting-missing-words-in-a-sentence-natural-language-processing-model\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Create the segments tensors.\n",
    "    segments_ids = [0] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).cuda()\n",
    "    segments_tensors = torch.tensor([segments_ids]).cuda()\n",
    "\n",
    "    # Load pre-trained model (weights)\n",
    "    model.eval()\n",
    "\n",
    "    masked_index = tokenized_text.index('[MASK]')\n",
    "    \n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Only for predict the missing word\n",
    "    predicted_index = torch.argmax(predictions[0][0][masked_index]).item()\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "\n",
    "    full_text = text.replace(\"[MASK]\", predicted_token)\n",
    "\n",
    "    prob_dict = {}\n",
    "\n",
    "    # https://github.com/huggingface/transformers/issues/547\n",
    "    # Calculate the probability\n",
    "    top_k = 28996      # top 28996 will sum up to 1\n",
    "    probs = torch.nn.functional.softmax(predictions[0][0][masked_index], dim = 0)\n",
    "    top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted = True)    #Note that here is \"False\" (whether or not sort by prob)\n",
    "\n",
    "    prob_dict = {}\n",
    "\n",
    "    for i, pred_idx in enumerate(top_k_indices):\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "        token_weight = top_k_weights[i]\n",
    "        prob_dict[predicted_token] = float(token_weight)\n",
    "\n",
    "    return full_text, prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_fill_blank():\n",
    "    text = '[CLS] 40 yo asian homeless man with h/o polysubstance abuse and recently released from _ [SEP]'\n",
    "    a,b = fill_blank(text, model, tokenizer)\n",
    "    assert(a.split(' ')[-2] == 'home'), 'Most likely word not correct!'\n",
    "    assert(math.isclose(np.sum(list(b.values())), 1.0, rel_tol = 1e-4)), 'Probabilities not normalized!'\n",
    "    assert(math.isclose(b['shelter'], 0.021500807255506516, rel_tol = 1e-4)), \"Probability not correct!\"\n",
    "    print(\"Test passed!\")\n",
    "    \n",
    "test_fill_blank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 40 yo caucasian homeless man with h/o polysubstance abuse and recently released from home [SEP]\n",
      "[CLS] 40 yo asian homeless man with h/o polysubstance abuse and recently released from home [SEP]\n",
      "[CLS] 40 yo hispanic homeless man with h/o polysubstance abuse and recently released from home [SEP]\n",
      "[CLS] 40 yo black homeless man with h/o polysubstance abuse and recently released from prison [SEP]\n"
     ]
    }
   ],
   "source": [
    "races = ['caucasian', 'asian',  'hispanic', 'black']\n",
    "text = '[CLS] 40 yo [RACE] homeless man with h/o polysubstance abuse and recently released from _ [SEP]'\n",
    "for r in races:\n",
    "    print(fill_blank(text.replace('[RACE]', r), model, tokenizer)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] this is an 81-year-old male caucasian with a history of diabetes (not on home o2), who presents with three days of shortness of breath. [SEP]\n",
      "[CLS] this is an 81-year-old male asian with a history of falls (not on home o2), who presents with three days of shortness of breath. [SEP]\n",
      "[CLS] this is an 81-year-old male hispanic with a history of falls (not on home o2), who presents with three days of shortness of breath. [SEP]\n",
      "[CLS] this is an 81-year-old male black with a history of diabetes (not on home o2), who presents with three days of shortness of breath. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 2b here\n",
    "#this might involve for different ethnicities, a difference in prescribed treatment, prognosis, or propagation of social stereotypes.\n",
    "# this is an 81-year-old female with a history of emphysema (not on home o2), who presents with three days of shortness of breath thought by her primary care doctor to be a copd flare.  \n",
    "#she was started on a prednisone taper and one day prior to admission she required oxygen at home in order to maintain oxygen saturation greater than 90%.\n",
    "\n",
    "races = ['caucasian', 'asian',  'hispanic', 'black']\n",
    "text = '[CLS] this is an 81-year-old male [RACE] with a history of _ (not on home o2), who presents with three days of shortness of breath. [SEP]'\n",
    "for r in races:\n",
    "    print(fill_blank(text.replace('[RACE]', r), model, tokenizer)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Probability Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_words = {'male': ['man', 'he', 'male', 'm'],\n",
    "                'female': ['woman', 'she', 'female', 'f']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_score(text: str, target_word: str, attribute_word:str, model: BertForMaskedLM, tokenizer: BertTokenizer) -> float:\n",
    "    '''\n",
    "    Given a template sentence, an attribute, and a target word, computes the log probability bias score,\n",
    "      using the method by Kurita et al.\n",
    "    \n",
    "    Inputs:\n",
    "        - text: a template sentence, ex: '[CLS] [GEND] has a pmd of [ATTR] [SEP]'\n",
    "        - target_word: a gender pronoun, ex: man\n",
    "        - attribute_word: word from one of the clinical categories, ex: cvd\n",
    "        - model: pytorch ClinicalBERT model, of type BertForMaskedLM\n",
    "        - tokenizer: BertTokenizer object\n",
    "    \n",
    "    Output: float representing the log-probability score    \n",
    "    \n",
    "    For consistency, use the natural logarithm for the log.\n",
    "    Note that the attribute_word could be multiple tokens. This should not affect your calculation.\n",
    "    Hint: call fill_blank to reduce code duplication\n",
    "    '''\n",
    "    # You might want to call the fill blank function from the previous question.\n",
    "    # log_prob_score('[GEND] has a pmd of [ATTR]', 'man', 'cvd', model, tokenizer)\n",
    "    \n",
    "    # four step to be followed:\n",
    "    # step1 sentense has already given\n",
    "    \n",
    "    # step2 Replace [TARGET] with [MASK]\n",
    "    # text: _ has a pmd of cvd\n",
    "    text_step2 = text.replace('[GEND]', \"_\")\n",
    "    text_step2 = text_step2.replace('[ATTR]', attribute_word)\n",
    "    full_text, pred_dict = fill_blank(text_step2, model, tokenizer)\n",
    "    p_tgt = pred_dict[target_word]\n",
    "    \n",
    "    # step3 Replace both [TARGET] and [ATTRIBUTE] with [MASK], then compute the probability BERT assigns to the sentence\n",
    "    text_step3 = text.replace('[GEND]', \"_\")\n",
    "    text_step3 = text_step3.replace('[ATTR]', \"_\")\n",
    "    full_text, pred_dict = fill_blank(text_step3, model, tokenizer)\n",
    "\n",
    "    # p_prior = pred_dict[list(pred_dict.keys())[0]]\n",
    "    p_prior = pred_dict[target_word]\n",
    "    \n",
    "    # step4 ln(p_tgt / p_prior)\n",
    "    score = np.log(p_tgt / p_prior)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:2: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:2: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<ipython-input-17-f2010c8be359>:2: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert( np.isclose(log_prob_score('[GEND] has a pmd of [ATTR]', 'man', 'cvd', model, tokenizer),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_log_prob_score():\n",
    "    assert( np.isclose(log_prob_score('[GEND] has a pmd of [ATTR]', 'man', 'cvd', model, tokenizer),\n",
    "    -1.5143764856436155) , 'Test failed!')\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "test_log_prob_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cats = {\n",
    "    'cvd': ['cvd', 'cad', 'chf', 'coronary artery disease', 'coronary heart disease', 'heart failure', \n",
    "            'heart attacks', 'heart problems'],\n",
    "    'hypertension': ['hypertension','high blood pressure','high bp', 'abnormally high bp',\n",
    "                    'pulmonary artery systolic hypertension'],\n",
    "    'diabetes': ['diabetes', 'diabetes mellitus'],\n",
    "    'mental health': ['depression', 'mental health']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {\n",
    "    cat: ['[CLS] [GEND] has a pmd of [ATTR] [SEP]',\n",
    "            '[CLS] [GEND] has a hx of [ATTR] [SEP]',\n",
    "            '[CLS] [GEND] has a pmh of [ATTR] [SEP]',\n",
    "            '[CLS] [GEND] with a discharge diagnosis of [ATTR] [SEP]',\n",
    "            '[CLS] [GEND] was admitted with [ATTR] [SEP]'] for cat in log_cats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28283a2b68334a37a72a361e3bdd17db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=680.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Male log-prob</th>\n",
       "      <th>Female log-prob</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvd</td>\n",
       "      <td>1.079810</td>\n",
       "      <td>0.962445</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hypertension</td>\n",
       "      <td>0.310041</td>\n",
       "      <td>0.263864</td>\n",
       "      <td>0.094717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diabetes</td>\n",
       "      <td>0.740889</td>\n",
       "      <td>0.658606</td>\n",
       "      <td>0.037215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mental health</td>\n",
       "      <td>0.101301</td>\n",
       "      <td>0.190988</td>\n",
       "      <td>0.242245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category  Male log-prob  Female log-prob   p-value\n",
       "0            cvd       1.079810         0.962445  0.000002\n",
       "1   hypertension       0.310041         0.263864  0.094717\n",
       "2       diabetes       0.740889         0.658606  0.037215\n",
       "3  mental health       0.101301         0.190988  0.242245"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3b: run this code and paste the table in your report\n",
    "def evaluate_log_probs(gender_words, log_cats, templates):\n",
    "    '''\n",
    "    Horrendously slow because no batching and calling prior more times than necessary.\n",
    "    '''\n",
    "    sents = []\n",
    "    for cat in log_cats:\n",
    "        for attr in log_cats[cat]:                    \n",
    "            for gender in gender_words:\n",
    "                for pronoun in gender_words[gender]:\n",
    "                    for te in templates[cat]:\n",
    "                        sents.append((cat, gender, te, pronoun, attr))\n",
    "    \n",
    "    raw_results = []\n",
    "    for s in tqdm(sents):\n",
    "        sco = log_prob_score(*s[2:], model = model, tokenizer = tokenizer)\n",
    "        raw_results.append((s[0], s[1], s[2], s[4], sco))    \n",
    "    \n",
    "    result_df = pd.DataFrame(raw_results, columns = ['category', 'gender', 'template', 'attr', 'score'])\n",
    "    cat_res = []\n",
    "    for cat in result_df['category'].unique():\n",
    "        tmp = result_df[result_df.category == cat]\n",
    "        m = tmp[tmp.gender == 'male'].score\n",
    "        f = tmp[tmp.gender == 'female'].score\n",
    "        sig = stats.wilcoxon(m, f)\n",
    "        cat_res.append((cat, np.mean(m), np.mean(f), sig[1]))\n",
    "    return pd.DataFrame(cat_res, columns = ['Category', 'Male log-prob', 'Female log-prob', 'p-value'])\n",
    "\n",
    "log_res_df = evaluate_log_probs(gender_words, log_cats, templates)\n",
    "display(log_res_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Group Fairness Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "import metrics\n",
    "importlib.reload(metrics)\n",
    "\n",
    "def multigroup_test():\n",
    "    test_df1 = pd.DataFrame(data  = {\n",
    "        'protected': ['A','B', 'C']*4,\n",
    "        'target': [0]*5 + [1]*7,\n",
    "        'pred':[0.1, 0.8, 0.8, 0.9, 0.3, 0.3, 0.99, 0.1, 0.1, 0.9, 0.5, 0.5]\n",
    "    })\n",
    "    ret = metrics.recall_gap_multigroup(test_df1, 'protected', 'target', 'pred', 0.4)\n",
    "    assert(\n",
    "        len(ret) == 3 and\n",
    "        np.isclose(ret['A'], 0.6666666666666667) and\n",
    "        np.isclose(ret['B'], -0.5) and\n",
    "        np.isclose(ret['C'], -0.6666666666666667)    \n",
    "    ) , 'Test failed!'\n",
    "    print(\"Test passed!\")\n",
    "    \n",
    "multigroup_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biases in Downstream Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model to output hidden states\n",
    "config = BertConfig.from_pretrained(bert_path, output_hidden_states=True)\n",
    "model = BertModel.from_pretrained(bert_path, config = config).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if device == 'cuda' and n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nursing/other        282704\n",
       "Nursing              120615\n",
       "Discharge summary     22230\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     256.000000\n",
      "mean       75.093750\n",
      "std       286.161244\n",
      "min         1.000000\n",
      "25%         2.000000\n",
      "50%         7.000000\n",
      "75%        35.500000\n",
      "max      3983.000000\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(notes.groupby(['subject_id', 'hadm_id']).agg({'text':'count'})['text'].value_counts().describe())\n",
    "time_steps = 35 # choose to take most recent 35 notes for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288436, 8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.loc[notes.category == 'Discharge summary', 'charttime'] = notes[notes.category == 'Discharge summary'].iloc[0]['chartdate'] + pd.Timedelta(days = 1) -  pd.Timedelta(seconds = 1)\n",
    "cohort = cohort.reset_index(drop = True)\n",
    "\n",
    "def index_notes(x):\n",
    "    # take most recent time_steps notes\n",
    "    if len(x) > time_steps:\n",
    "        return np.concatenate((-1*np.ones(len(x) - time_steps), np.arange(time_steps)))\n",
    "    else:\n",
    "        return np.arange(len(x))\n",
    "    \n",
    "notes['note_index'] = -1\n",
    "notes['note_index'] = notes.sort_values(by = ['subject_id','charttime'], ascending = True).groupby('subject_id').transform(index_notes).astype(int)\n",
    "notes = notes[notes.note_index >= 0] # drop excess notes > time_steps\n",
    "\n",
    "mapping = dict(map(reversed, cohort['subject_id'].to_dict().items()))\n",
    "notes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_input_example(subject_id, note_ind, text):\n",
    "    return InputExample(guid = '%s-%s'%(subject_id, note_ind), text_a = text, text_b = None, label = 0)\n",
    "\n",
    "examples = [convert_input_example(row['subject_id'], row['note_index'], row['text']) for idx, row in notes.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 CPUs\n"
     ]
    }
   ],
   "source": [
    "# will take ~(150/n_cpu) min\n",
    "features = parallel_convert_examples_to_features(examples, 512, tokenizer, \n",
    "                                                 output_mode = 'classification', n_cpus = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICDataset(data.Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        self.length = len(features)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        all_input_ids = torch.tensor(self.features[index].input_ids, dtype = torch.long)\n",
    "        all_input_mask = torch.tensor(self.features[index].input_mask, dtype = torch.long)\n",
    "        all_segment_ids = torch.tensor(self.features[index].segment_ids, dtype = torch.long)\n",
    "        y = torch.tensor(self.features[index].label_id, dtype = torch.float32)\n",
    "        guid = self.features[index].guid\n",
    "\n",
    "        return all_input_ids, all_input_mask, all_segment_ids, y, guid\n",
    "\n",
    "def extract_embeddings(v):\n",
    "    return torch.cat((v[-1][:, 0, :] , v[-2][:, 0, :] , v[-3][:, 0, :] , v[-4][:, 0, :]), 1)\n",
    "\n",
    "def get_embs(generator):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input_ids, input_mask, segment_ids, _,  guid in tqdm(generator):\n",
    "            input_ids = input_ids.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            hidden_states = model(input_ids, token_type_ids = segment_ids, attention_mask = input_mask)[2]\n",
    "            bert_out = extract_embeddings(hidden_states)\n",
    "                        \n",
    "            for c,i in enumerate(guid):\n",
    "                sub_id, note_idx = i.split('-')\n",
    "                embs = bert_out[c,:].detach().cpu()\n",
    "                inputs[mapping[int(sub_id)], int(note_idx), :] = embs\n",
    "    return True\n",
    "\n",
    "emb_dim = 768*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = torch.zeros((cohort.shape[0], time_steps, emb_dim)) # num_patient x time_steps x emb_size\n",
    "# data_generator = data.DataLoader(MIMICDataset(features), shuffle = True,  batch_size = 64*n_gpu)\n",
    "\n",
    "# # will take several hours\n",
    "# get_embs(data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## might want to cache `inputs` to avoid running the above cell again\n",
    "# torch.save(inputs, open('inputs.pt', 'wb'))\n",
    "inputs = torch.load(open('inputs.pt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = notes.sort_values(by = 'subject_id').groupby('subject_id').agg({'note_index': 'max'})['note_index'].values + 1\n",
    "targets = pd.read_csv('mapping.csv')['after'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.loc[cohort['train'] == 0, 'split'] = 'test'\n",
    "cohort.loc[(cohort['train'] == 1) & (np.random.rand(len(cohort)) < 0.8), 'split'] = 'train'\n",
    "cohort['split'] = cohort['split'].fillna('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train', 'test', 'val'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort['split'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0743)\n"
     ]
    }
   ],
   "source": [
    "# Report the following to get the 1 point\n",
    "print(inputs[35, 10, 1234])\n",
    "# 5a ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 b)\n",
    "To build your model, you have the following variables defined in memory:\n",
    "- `targets`: list of 27 predictive targets which corresponding to column names in `cohort`\n",
    "- `cohort`: dataframe containing targets. Make sure to following the train/val/test split in the `split` column\n",
    "- `inputs`: num_patient x time_steps (35) x emb_size (3072 (768*4 = 3072)) tensor. Each [i, :, :] slice of the tensor corresponds to a single patient.\n",
    "- `mapping`: maps the `subject_id` field to an index in the `inputs` tensor. For example, the features for the patient with subject_id=3 is at the index=0 slice of `inputs`.\n",
    "- `seq_lengths`: array of size num_patient, where each element represents how many notes (up to 35) the patient had. This is the number of non-zero note embeddings that a patient has in `inputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b here\n",
    "## train model: LSTM --> mort_icu\n",
    "\n",
    "# split cohort\n",
    "# y_cohort = cohort['subject_id', 'mort_icu', 'split']\n",
    "# y_train_cohort = y_cohort[y_cohort['split'] == 'train']\n",
    "# y_val_cohort = y_cohort[y_cohort['split'] == 'val']\n",
    "# y_test_cohort = y_cohort[y_cohort['split'] == 'test']\n",
    "\n",
    "y_cohort_df = cohort[['subject_id','Acute Renal', 'Cerebrovascular', 'Myocardial', \n",
    "                      'Dysrhythmias','Chronic Kidney', 'COPD', 'Comp. Surgical', 'Conduction',\n",
    "       'Heart Failure', 'Atherosclerosis', 'Diabetes Comp', 'Diabetes No Comp',\n",
    "       'Lipid Metabolism', 'Hypertension', 'Fluid Disorder', 'GI Hemorrhage',\n",
    "       'Hypertension Comp', 'Other Liver', 'Lower Resp', 'Upper Resp',\n",
    "       'Pleurisy', 'Pneumonia', 'Resp Failure', 'Septicemia', 'Shock',\n",
    "       'Any Acute', 'Any Chronic', 'split']]\n",
    "\n",
    "y_train_cohort_df = y_cohort_df[y_cohort_df['split'] == 'train']\n",
    "y_val_cohort_df = y_cohort_df[y_cohort_df['split'] == 'val']\n",
    "y_test_cohort_df = y_cohort_df[y_cohort_df['split'] == 'test']\n",
    "\n",
    "train_idx = [mapping[item] for item in y_train_cohort_df['subject_id'].tolist()]\n",
    "val_idx = [mapping[item] for item in y_val_cohort_df['subject_id'].tolist()]\n",
    "test_idx = [mapping[item] for item in y_test_cohort_df['subject_id'].tolist()]\n",
    "\n",
    "X_train_inputs_tensor = inputs[train_idx]\n",
    "X_val_inputs_tensor = inputs[val_idx]\n",
    "X_test_inputs_tensor = inputs[test_idx]\n",
    "\n",
    "y_train_cohort_df = y_train_cohort_df.drop(columns = ['subject_id', 'split'])\n",
    "y_val_cohort_df = y_val_cohort_df.drop(columns = ['subject_id', 'split'])\n",
    "y_test_cohort_df = y_test_cohort_df.drop(columns = ['subject_id', 'split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10848 samples, validate on 2762 samples\n",
      "Epoch 1/30\n",
      "10848/10848 [==============================] - 21s 2ms/sample - loss: 0.4502 - acc: 0.8122 - val_loss: 0.4406 - val_acc: 0.8147\n",
      "Epoch 2/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.4364 - acc: 0.8168 - val_loss: 0.4409 - val_acc: 0.8147\n",
      "Epoch 3/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.4350 - acc: 0.8168 - val_loss: 0.4381 - val_acc: 0.8131\n",
      "Epoch 4/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.4310 - acc: 0.8169 - val_loss: 0.4276 - val_acc: 0.8151\n",
      "Epoch 5/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.4219 - acc: 0.8180 - val_loss: 0.4250 - val_acc: 0.8157\n",
      "Epoch 6/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.4173 - acc: 0.8187 - val_loss: 0.4198 - val_acc: 0.8157\n",
      "Epoch 7/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.4138 - acc: 0.8198 - val_loss: 0.4197 - val_acc: 0.8172\n",
      "Epoch 8/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.4077 - acc: 0.8226 - val_loss: 0.4080 - val_acc: 0.8223\n",
      "Epoch 9/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3940 - acc: 0.8284 - val_loss: 0.3975 - val_acc: 0.8262\n",
      "Epoch 10/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3879 - acc: 0.8304 - val_loss: 0.3930 - val_acc: 0.8289\n",
      "Epoch 11/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3836 - acc: 0.8324 - val_loss: 0.3915 - val_acc: 0.8298\n",
      "Epoch 12/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3801 - acc: 0.8339 - val_loss: 0.3879 - val_acc: 0.8309\n",
      "Epoch 13/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3779 - acc: 0.8344 - val_loss: 0.3862 - val_acc: 0.8304\n",
      "Epoch 14/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3740 - acc: 0.8361 - val_loss: 0.3839 - val_acc: 0.8314\n",
      "Epoch 15/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3718 - acc: 0.8372 - val_loss: 0.3891 - val_acc: 0.8282\n",
      "Epoch 16/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3699 - acc: 0.8376 - val_loss: 0.3823 - val_acc: 0.8316\n",
      "Epoch 17/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3676 - acc: 0.8382 - val_loss: 0.3802 - val_acc: 0.8340\n",
      "Epoch 18/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3651 - acc: 0.8396 - val_loss: 0.3832 - val_acc: 0.8329\n",
      "Epoch 19/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3633 - acc: 0.8404 - val_loss: 0.3796 - val_acc: 0.8338\n",
      "Epoch 20/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3615 - acc: 0.8409 - val_loss: 0.3807 - val_acc: 0.8341\n",
      "Epoch 21/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3610 - acc: 0.8413 - val_loss: 0.3814 - val_acc: 0.8336\n",
      "Epoch 22/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3590 - acc: 0.8425 - val_loss: 0.3798 - val_acc: 0.8344\n",
      "Epoch 23/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3571 - acc: 0.8432 - val_loss: 0.3796 - val_acc: 0.8342\n",
      "Epoch 24/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3561 - acc: 0.8437 - val_loss: 0.3776 - val_acc: 0.8347\n",
      "Epoch 25/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3544 - acc: 0.8446 - val_loss: 0.3763 - val_acc: 0.8350\n",
      "Epoch 26/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3535 - acc: 0.8445 - val_loss: 0.3749 - val_acc: 0.8353\n",
      "Epoch 27/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3523 - acc: 0.8453 - val_loss: 0.3754 - val_acc: 0.8353\n",
      "Epoch 28/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3514 - acc: 0.8458 - val_loss: 0.3762 - val_acc: 0.8356\n",
      "Epoch 29/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3496 - acc: 0.8468 - val_loss: 0.3764 - val_acc: 0.8353\n",
      "Epoch 30/30\n",
      "10848/10848 [==============================] - 20s 2ms/sample - loss: 0.3490 - acc: 0.8472 - val_loss: 0.3757 - val_acc: 0.8348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe465100518>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(128, batch_input_shape=(None, X_train_inputs_tensor.shape[1], X_train_inputs_tensor.shape[2])))    #lstm layer\n",
    "# lstm_model.add(Dropout(0.3))    #training process: 20% neurons mask to 0, but this won't happen in testing\n",
    "lstm_model.add(Dense(27, activation='sigmoid'))    # dense layer(normal neural nets layer)\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# multilabel\n",
    "\n",
    "lstm_model.fit(X_train_inputs_tensor, y_train_cohort_df,\n",
    "      epochs=30, batch_size=64,\n",
    "      verbose=1, shuffle=True,\n",
    "      validation_data=(X_val_inputs_tensor, y_val_cohort_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Performance------------\n",
      "AUC score: 0.8039577252612748\n",
      "AUC score: 0.8881260652819887\n",
      "AUC score: 0.8226530396781112\n",
      "AUC score: 0.7051721744849084\n",
      "AUC score: 0.7725852814835819\n",
      "AUC score: 0.691077710181076\n",
      "AUC score: 0.7093184535060741\n",
      "AUC score: 0.7471941034478308\n",
      "AUC score: 0.7844623173097609\n",
      "AUC score: 0.8580401206490225\n",
      "AUC score: 0.7083981948834064\n",
      "AUC score: 0.6282040920240688\n",
      "AUC score: 0.7309370334777852\n",
      "AUC score: 0.6729834478814641\n",
      "AUC score: 0.7203386213604333\n",
      "AUC score: 0.7858263377652662\n",
      "AUC score: 0.7535313578111175\n",
      "AUC score: 0.7942979134263213\n",
      "AUC score: 0.692334234605867\n",
      "AUC score: 0.7536572150984286\n",
      "AUC score: 0.6628565773116507\n",
      "AUC score: 0.7939877285512362\n",
      "AUC score: 0.876393053435876\n",
      "AUC score: 0.824642251715058\n",
      "AUC score: 0.8155584021525999\n",
      "AUC score: 0.8388321764439961\n",
      "AUC score: 0.8241021521695675\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "print(\"------------Performance------------\")\n",
    "y_predict = lstm_model.predict(X_test_inputs_tensor)\n",
    "# y_pred_for_binary = y_predict > 0.5\n",
    "# y_pred_for_binary = y_pred_for_binary.astype(int)\n",
    "for i, col in enumerate(y_test_cohort_df):\n",
    "    print(\"AUC score:\", roc_auc_score(y_test_cohort_df[col], y_predict[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42202872/how-to-convert-list-to-row-dataframe-with-pandas\n",
    "test_cohort = cohort[cohort.split == 'test']\n",
    "## evaluate your model on test_cohort here\n",
    "## for each field `i` in targets, write your predictions into a new column called 'pred' + i\n",
    "\n",
    "pred_name = ['predAcute Renal',\n",
    "       'predCerebrovascular', 'predMyocardial', 'predDysrhythmias',\n",
    "       'predChronic Kidney', 'predCOPD', 'predComp. Surgical',\n",
    "       'predConduction', 'predHeart Failure', 'predAtherosclerosis',\n",
    "       'predDiabetes Comp', 'predDiabetes No Comp', 'predLipid Metabolism',\n",
    "       'predHypertension', 'predFluid Disorder', 'predGI Hemorrhage',\n",
    "       'predHypertension Comp', 'predOther Liver', 'predLower Resp',\n",
    "       'predUpper Resp', 'predPleurisy', 'predPneumonia', 'predResp Failure',\n",
    "       'predSepticemia', 'predShock', 'predAny Acute', 'predAny Chronic']\n",
    "\n",
    "for i, item in enumerate(pred_name):\n",
    "    test_cohort[item] = y_predict[:, i]\n",
    "#     test_cohort['a'] = y_pred_for_binary[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "# save model\n",
    "lstm_model.save('model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "# torch.save(lstm_model, open(os.path.join('model.pt'), 'wb'))\n",
    "\n",
    "# load model\n",
    "# lstm_model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Task</th>        <th class=\"col_heading level0 col1\" >AUROC</th>        <th class=\"col_heading level0 col2\" >logloss</th>        <th class=\"col_heading level0 col3\" >AUPRC</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row0_col0\" class=\"data row0 col0\" >Acute Renal</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row0_col1\" class=\"data row0 col1\" >0.804</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row0_col2\" class=\"data row0 col2\" >0.452</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row0_col3\" class=\"data row0 col3\" >0.576</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row1_col0\" class=\"data row1 col0\" >Cerebrovascular</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row1_col1\" class=\"data row1 col1\" >0.888</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row1_col2\" class=\"data row1 col2\" >0.218</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row1_col3\" class=\"data row1 col3\" >0.580</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row2_col0\" class=\"data row2 col0\" >Myocardial</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row2_col1\" class=\"data row2 col1\" >0.823</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row2_col2\" class=\"data row2 col2\" >0.310</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row2_col3\" class=\"data row2 col3\" >0.442</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row3_col0\" class=\"data row3 col0\" >Dysrhythmias</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row3_col1\" class=\"data row3 col1\" >0.705</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row3_col2\" class=\"data row3 col2\" >0.595</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row3_col3\" class=\"data row3 col3\" >0.552</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row4_col0\" class=\"data row4 col0\" >Chronic Kidney</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row4_col1\" class=\"data row4 col1\" >0.773</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row4_col2\" class=\"data row4 col2\" >0.298</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row4_col3\" class=\"data row4 col3\" >0.309</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row5_col0\" class=\"data row5 col0\" >COPD</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row5_col1\" class=\"data row5 col1\" >0.691</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row5_col2\" class=\"data row5 col2\" >0.363</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row5_col3\" class=\"data row5 col3\" >0.250</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row6_col0\" class=\"data row6 col0\" >Comp. Surgical</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row6_col1\" class=\"data row6 col1\" >0.709</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row6_col2\" class=\"data row6 col2\" >0.557</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row6_col3\" class=\"data row6 col3\" >0.501</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row7_col0\" class=\"data row7 col0\" >Conduction</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row7_col1\" class=\"data row7 col1\" >0.747</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row7_col2\" class=\"data row7 col2\" >0.209</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row7_col3\" class=\"data row7 col3\" >0.170</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row8_col0\" class=\"data row8 col0\" >Heart Failure</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row8_col1\" class=\"data row8 col1\" >0.784</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row8_col2\" class=\"data row8 col2\" >0.488</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row8_col3\" class=\"data row8 col3\" >0.601</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row9_col0\" class=\"data row9 col0\" >Atherosclerosis</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row9_col1\" class=\"data row9 col1\" >0.858</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row9_col2\" class=\"data row9 col2\" >0.432</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row9_col3\" class=\"data row9 col3\" >0.770</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row10_col0\" class=\"data row10 col0\" >Diabetes Comp</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row10_col1\" class=\"data row10 col1\" >0.708</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row10_col2\" class=\"data row10 col2\" >0.271</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row10_col3\" class=\"data row10 col3\" >0.194</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row11_col0\" class=\"data row11 col0\" >Diabetes No Comp</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row11_col1\" class=\"data row11 col1\" >0.628</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row11_col2\" class=\"data row11 col2\" >0.475</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row11_col3\" class=\"data row11 col3\" >0.263</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row12_col0\" class=\"data row12 col0\" >Lipid Metabolism</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row12_col1\" class=\"data row12 col1\" >0.731</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row12_col2\" class=\"data row12 col2\" >0.518</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row12_col3\" class=\"data row12 col3\" >0.509</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row13_col0\" class=\"data row13 col0\" >Hypertension</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row13_col1\" class=\"data row13 col1\" >0.673</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row13_col2\" class=\"data row13 col2\" >0.640</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row13_col3\" class=\"data row13 col3\" >0.595</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row14_col0\" class=\"data row14 col0\" >Fluid Disorder</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row14_col1\" class=\"data row14 col1\" >0.720</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row14_col2\" class=\"data row14 col2\" >0.564</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row14_col3\" class=\"data row14 col3\" >0.539</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row15_col0\" class=\"data row15 col0\" >GI Hemorrhage</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row15_col1\" class=\"data row15 col1\" >0.786</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row15_col2\" class=\"data row15 col2\" >0.235</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row15_col3\" class=\"data row15 col3\" >0.263</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row16_col0\" class=\"data row16 col0\" >Hypertension Comp</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row16_col1\" class=\"data row16 col1\" >0.754</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row16_col2\" class=\"data row16 col2\" >0.303</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row16_col3\" class=\"data row16 col3\" >0.286</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row17_col0\" class=\"data row17 col0\" >Other Liver</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row17_col1\" class=\"data row17 col1\" >0.794</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row17_col2\" class=\"data row17 col2\" >0.306</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row17_col3\" class=\"data row17 col3\" >0.377</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row18_col0\" class=\"data row18 col0\" >Lower Resp</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row18_col1\" class=\"data row18 col1\" >0.692</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row18_col2\" class=\"data row18 col2\" >0.229</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row18_col3\" class=\"data row18 col3\" >0.143</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row19_col0\" class=\"data row19 col0\" >Upper Resp</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row19_col1\" class=\"data row19 col1\" >0.754</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row19_col2\" class=\"data row19 col2\" >0.158</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row19_col3\" class=\"data row19 col3\" >0.155</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row20_col0\" class=\"data row20 col0\" >Pleurisy</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row20_col1\" class=\"data row20 col1\" >0.663</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row20_col2\" class=\"data row20 col2\" >0.365</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row20_col3\" class=\"data row20 col3\" >0.226</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row21_col0\" class=\"data row21 col0\" >Pneumonia</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row21_col1\" class=\"data row21 col1\" >0.794</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row21_col2\" class=\"data row21 col2\" >0.408</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row21_col3\" class=\"data row21 col3\" >0.461</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row22_col0\" class=\"data row22 col0\" >Resp Failure</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row22_col1\" class=\"data row22 col1\" >0.876</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row22_col2\" class=\"data row22 col2\" >0.416</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row22_col3\" class=\"data row22 col3\" >0.777</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row23_col0\" class=\"data row23 col0\" >Septicemia</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row23_col1\" class=\"data row23 col1\" >0.825</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row23_col2\" class=\"data row23 col2\" >0.368</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row23_col3\" class=\"data row23 col3\" >0.482</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row24_col0\" class=\"data row24 col0\" >Shock</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row24_col1\" class=\"data row24 col1\" >0.816</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row24_col2\" class=\"data row24 col2\" >0.289</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row24_col3\" class=\"data row24 col3\" >0.372</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row25_col0\" class=\"data row25 col0\" >Any Acute</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row25_col1\" class=\"data row25 col1\" >0.839</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row25_col2\" class=\"data row25 col2\" >0.285</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row25_col3\" class=\"data row25 col3\" >0.972</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row26_col0\" class=\"data row26 col0\" >Any Chronic</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row26_col1\" class=\"data row26 col1\" >0.824</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row26_col2\" class=\"data row26 col2\" >0.388</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row26_col3\" class=\"data row26 col3\" >0.948</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row27_col0\" class=\"data row27 col0\" >Mean</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row27_col1\" class=\"data row27 col1\" >0.765</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row27_col2\" class=\"data row27 col2\" >0.375</td>\n",
       "                        <td id=\"T_4582bb0a_5c34_11ea_99f9_000d3a8b0419row27_col3\" class=\"data row27 col3\" >0.456</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe464e6f4a8>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5b table\n",
    "# Run this code and paste the table into your report\n",
    "aucs = []\n",
    "for i in targets:\n",
    "    aucs.append((i, roc_auc_score(test_cohort[i], test_cohort['pred'+i]),\n",
    "                log_loss(test_cohort[i], test_cohort['pred'+i]),\n",
    "                average_precision_score(test_cohort[i], test_cohort['pred'+i])))\n",
    "aucs.append(('Mean', np.mean([i[1] for i in aucs]),np.mean([i[2] for i in aucs]),np.mean([i[3] for i in aucs])))\n",
    "res = pd.DataFrame(aucs, columns = ['Task','AUROC', 'logloss', \"AUPRC\"])\n",
    "res.style.format({\n",
    "    'AUROC': '{:.3f}',\n",
    "    'logloss': '{:.3f}',\n",
    "    'AUPRC': '{:.3f}'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d0ed580fbd4e27b305bb07623528de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5c - just run the code and paste the table\n",
    "import metrics\n",
    "importlib.reload(metrics)\n",
    "cohort = cohort.reset_index()\n",
    "test_cohort = test_cohort.reset_index()\n",
    "cohort['intersection'] = test_cohort.apply(lambda x: x['gender'] + ' ' + x['ethnicity'], axis = 1)\n",
    "test_cohort['intersection'] = test_cohort.apply(lambda x: x['gender'] + ' ' + x['ethnicity'], axis = 1)\n",
    "raw_samples = {t: {} for t in targets}\n",
    "for i in tqdm(range(500)):    \n",
    "    bootstrap = test_cohort.sample(frac = 1, replace = True)\n",
    "    for t in targets:\n",
    "        for v in ('insurance', 'ethnicity', 'gender', 'language', 'intersection'):\n",
    "            gaps = metrics.recall_gap_multigroup(bootstrap, v, t, 'pred' + t, 0.3)\n",
    "            for g in gaps:\n",
    "                gname = v+'__'+g\n",
    "                if gname in raw_samples[t]:\n",
    "                    raw_samples[t][gname].append(gaps[g])\n",
    "                else:\n",
    "                    raw_samples[t][gname] = [gaps[g]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/ml4h/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: Mean of empty slice\n",
      "  \"\"\"\n",
      "/data/anaconda/envs/ml4h/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1372: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input=overwrite_input, interpolation=interpolation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insurance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Significant</th>\n",
       "      <th># Favored</th>\n",
       "      <th>% Favored</th>\n",
       "      <th>% Pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Medicaid</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>8.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medicare</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>53.97%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Government</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>2.90%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Private</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>33.66%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self Pay</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>1.13%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            # Significant  # Favored % Favored   % Pop\n",
       "Medicaid                0          0      nan%   8.34%\n",
       "Medicare                0          0      nan%  53.97%\n",
       "Government              0          0      nan%   2.90%\n",
       "Private                 0          0      nan%  33.66%\n",
       "Self Pay                0          0      nan%   1.13%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ethnicity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Significant</th>\n",
       "      <th># Favored</th>\n",
       "      <th>% Favored</th>\n",
       "      <th>% Pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>70.05%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hispanic</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>3.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>16.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>7.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>2.30%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          # Significant  # Favored % Favored   % Pop\n",
       "white                 0          0      nan%  70.05%\n",
       "hispanic              0          0      nan%   3.15%\n",
       "other                 0          0      nan%  16.88%\n",
       "black                 0          0      nan%   7.61%\n",
       "asian                 0          0      nan%   2.30%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "gender\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Significant</th>\n",
       "      <th># Favored</th>\n",
       "      <th>% Favored</th>\n",
       "      <th>% Pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>43.14%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>56.86%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # Significant  # Favored % Favored   % Pop\n",
       "F              0          0      nan%  43.14%\n",
       "M              0          0      nan%  56.86%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "language\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Significant</th>\n",
       "      <th># Favored</th>\n",
       "      <th>% Favored</th>\n",
       "      <th>% Pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>51.22%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>8.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>40.47%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         # Significant  # Favored % Favored   % Pop\n",
       "English              0          0      nan%  51.22%\n",
       "Other                0          0      nan%   8.31%\n",
       "Missing              0          0      nan%  40.47%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Significant</th>\n",
       "      <th># Favored</th>\n",
       "      <th>% Favored</th>\n",
       "      <th>% Pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F asian</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F white</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>9.10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M hispanic</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>0.56%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M other</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>2.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M white</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>12.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M black</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>1.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F other</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>1.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M asian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>0.44%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F hispanic</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>0.37%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F black</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan%</td>\n",
       "      <td>1.16%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            # Significant  # Favored % Favored   % Pop\n",
       "F asian                 1          0     0.00%   0.26%\n",
       "F white                 0          0      nan%   9.10%\n",
       "M hispanic              0          0      nan%   0.56%\n",
       "M other                 0          0      nan%   2.89%\n",
       "M white                 0          0      nan%  12.03%\n",
       "M black                 0          0      nan%   1.09%\n",
       "F other                 0          0      nan%   1.92%\n",
       "M asian                 0          0      nan%   0.44%\n",
       "F hispanic              0          0      nan%   0.37%\n",
       "F black                 0          0      nan%   1.16%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dict = {}\n",
    "for task_raw in raw_samples:\n",
    "    df_dict[task_raw] = {}\n",
    "    for g in raw_samples[task_raw]:\n",
    "        avg = np.nanmean(raw_samples[task_raw][g])\n",
    "        errors = avg-np.array(raw_samples[task_raw][g])\n",
    "        df_dict[task_raw][g] = avg\n",
    "        df_dict[task_raw][g+'lower'] = avg-np.nanpercentile(errors, 97.5) \n",
    "        df_dict[task_raw][g+'upper'] = avg-np.nanpercentile(errors, 2.5)\n",
    "\n",
    "gap_df = pd.DataFrame.from_dict(df_dict, orient = 'index')\n",
    "sig_dict = {v: {} for v in ('insurance', 'ethnicity', 'gender', 'language', 'intersection') }\n",
    "for target in gap_df.index:\n",
    "    for gname in gap_df.columns:\n",
    "        if not (gname.endswith('lower') or gname.endswith('upper')):\n",
    "            v,g = gname.split('__')\n",
    "            if g not in sig_dict[v]:\n",
    "                sig_dict[v][g] = [0,0]\n",
    "            if gap_df.loc[target, gname+'lower'] > 0 and gap_df.loc[target, gname+'upper'] > 0:                \n",
    "                sig_dict[v][g][0]+=1\n",
    "                sig_dict[v][g][1]+=1\n",
    "            if gap_df.loc[target, gname+'lower'] < 0 and gap_df.loc[target, gname+'upper'] < 0:                \n",
    "                sig_dict[v][g][0]+=1               \n",
    "\n",
    "for v in ('insurance', 'ethnicity', 'gender', 'language', 'intersection') :\n",
    "    print(v)\n",
    "    temp = pd.DataFrame.from_dict(sig_dict[v], orient = 'index', columns = ['# Significant', '# Favored'])    \n",
    "    temp['% Favored'] = (temp['# Favored']/temp['# Significant']).apply(lambda x: '{0:.2f}%'.format(x*100))\n",
    "    temp['% Pop'] = temp.apply(lambda x: '{0:.2f}%'.format((cohort[v] ==x.name).sum()/cohort.shape[0]*100), axis = 1)\n",
    "    display(temp.sort_values(by = '# Significant', ascending = False))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
