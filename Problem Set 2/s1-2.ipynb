{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Run the `create_data.py` script to generate the following files:\n",
    "\n",
    "- `cohort.csv`:\n",
    "  - Contains one record for each adult patient’s first ICU stay over 48 hours in lengthwithin their first hospital admission.\n",
    "  - The `mort_icu` column represents whether the patient died during their ICU stay.\n",
    "  - The columns from `Acute Renal` to `Shock` correspond to each of the 25 CCS code  groups, which are derived from  ICD-9 codes assigned at the end of a patient’s hospital stay.\n",
    "  - The `Any Acute` and `Any Chronic` columns are derived from whether the patient has any acute and chronic phenotypes respectively.\n",
    "  \n",
    "  \n",
    "- `notes.csv`\n",
    "  - Contains, for each of the patients in `cohort.csv`, all of the notes written during their hospital stay (along with the timestamp) for the following note types:\n",
    "      - Discharge Summary\n",
    "      - Nursing\n",
    "      - Nursing/other\n",
    "  - The notes have been lightly preprocessed (ex: removing PHI identifiers, removing section numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'mimic_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = pd.read_hdf(os.path.join(data_path, 'cohort.h5'))\n",
    "notes = pd.read_hdf(os.path.join(data_path, 'notes.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>admittime</th>\n",
       "      <th>dischtime</th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>language</th>\n",
       "      <th>insurance</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>mort_icu</th>\n",
       "      <th>intime</th>\n",
       "      <th>Acute Renal</th>\n",
       "      <th>Cerebrovascular</th>\n",
       "      <th>Myocardial</th>\n",
       "      <th>Dysrhythmias</th>\n",
       "      <th>Chronic Kidney</th>\n",
       "      <th>COPD</th>\n",
       "      <th>Comp. Surgical</th>\n",
       "      <th>Conduction</th>\n",
       "      <th>Heart Failure</th>\n",
       "      <th>Atherosclerosis</th>\n",
       "      <th>Diabetes Comp</th>\n",
       "      <th>Diabetes No Comp</th>\n",
       "      <th>Lipid Metabolism</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Fluid Disorder</th>\n",
       "      <th>GI Hemorrhage</th>\n",
       "      <th>Hypertension Comp</th>\n",
       "      <th>Other Liver</th>\n",
       "      <th>Lower Resp</th>\n",
       "      <th>Upper Resp</th>\n",
       "      <th>Pleurisy</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Resp Failure</th>\n",
       "      <th>Septicemia</th>\n",
       "      <th>Shock</th>\n",
       "      <th>Any Acute</th>\n",
       "      <th>Any Chronic</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>211552</td>\n",
       "      <td>M</td>\n",
       "      <td>2101-10-20 19:08:00</td>\n",
       "      <td>2101-10-31 13:58:00</td>\n",
       "      <td>76.0</td>\n",
       "      <td>white</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2101-10-20 19:10:11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>107064</td>\n",
       "      <td>228232</td>\n",
       "      <td>F</td>\n",
       "      <td>2175-05-30 07:15:00</td>\n",
       "      <td>2175-06-15 16:00:00</td>\n",
       "      <td>65.0</td>\n",
       "      <td>white</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>English</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2175-05-30 21:30:54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>150750</td>\n",
       "      <td>220597</td>\n",
       "      <td>M</td>\n",
       "      <td>2149-11-09 13:06:00</td>\n",
       "      <td>2149-11-14 10:15:00</td>\n",
       "      <td>41.0</td>\n",
       "      <td>other</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Medicaid</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2149-11-09 13:07:02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>112213</td>\n",
       "      <td>232669</td>\n",
       "      <td>M</td>\n",
       "      <td>2104-08-07 10:15:00</td>\n",
       "      <td>2104-08-20 02:57:00</td>\n",
       "      <td>72.0</td>\n",
       "      <td>white</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2104-08-08 02:08:17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>143045</td>\n",
       "      <td>263738</td>\n",
       "      <td>F</td>\n",
       "      <td>2167-01-08 18:43:00</td>\n",
       "      <td>2167-01-15 15:15:00</td>\n",
       "      <td>39.0</td>\n",
       "      <td>white</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Medicaid</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2167-01-08 18:44:25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  hadm_id  icustay_id gender           admittime  \\\n",
       "0           3   145834      211552      M 2101-10-20 19:08:00   \n",
       "1           6   107064      228232      F 2175-05-30 07:15:00   \n",
       "2           9   150750      220597      M 2149-11-09 13:06:00   \n",
       "3          12   112213      232669      M 2104-08-07 10:15:00   \n",
       "4          13   143045      263738      F 2167-01-08 18:43:00   \n",
       "\n",
       "            dischtime   age ethnicity admission_type language insurance  \\\n",
       "0 2101-10-31 13:58:00  76.0     white      EMERGENCY  Missing  Medicare   \n",
       "1 2175-06-15 16:00:00  65.0     white       ELECTIVE  English  Medicare   \n",
       "2 2149-11-14 10:15:00  41.0     other      EMERGENCY  Missing  Medicaid   \n",
       "3 2104-08-20 02:57:00  72.0     white       ELECTIVE  Missing  Medicare   \n",
       "4 2167-01-15 15:15:00  39.0     white      EMERGENCY  Missing  Medicaid   \n",
       "\n",
       "   hospital_expire_flag  mort_icu              intime  Acute Renal  \\\n",
       "0                     0         0 2101-10-20 19:10:11          1.0   \n",
       "1                     0         0 2175-05-30 21:30:54          0.0   \n",
       "2                     1         1 2149-11-09 13:07:02          1.0   \n",
       "3                     1         0 2104-08-08 02:08:17          0.0   \n",
       "4                     0         0 2167-01-08 18:44:25          0.0   \n",
       "\n",
       "   Cerebrovascular  Myocardial  Dysrhythmias  Chronic Kidney  COPD  \\\n",
       "0              0.0         1.0           0.0             0.0   0.0   \n",
       "1              0.0         0.0           0.0             0.0   0.0   \n",
       "2              1.0         0.0           0.0             0.0   0.0   \n",
       "3              0.0         0.0           0.0             0.0   0.0   \n",
       "4              0.0         0.0           0.0             0.0   0.0   \n",
       "\n",
       "   Comp. Surgical  Conduction  Heart Failure  Atherosclerosis  Diabetes Comp  \\\n",
       "0             0.0         0.0            1.0              0.0            0.0   \n",
       "1             1.0         0.0            0.0              0.0            0.0   \n",
       "2             0.0         0.0            1.0              0.0            0.0   \n",
       "3             1.0         0.0            0.0              0.0            0.0   \n",
       "4             0.0         0.0            0.0              1.0            0.0   \n",
       "\n",
       "   Diabetes No Comp  Lipid Metabolism  Hypertension  Fluid Disorder  \\\n",
       "0               0.0               0.0           0.0             0.0   \n",
       "1               0.0               0.0           0.0             1.0   \n",
       "2               0.0               0.0           1.0             1.0   \n",
       "3               0.0               0.0           1.0             0.0   \n",
       "4               1.0               1.0           1.0             0.0   \n",
       "\n",
       "   GI Hemorrhage  Hypertension Comp  Other Liver  Lower Resp  Upper Resp  \\\n",
       "0            0.0                0.0          0.0         0.0         0.0   \n",
       "1            0.0                1.0          0.0         0.0         0.0   \n",
       "2            0.0                0.0          0.0         0.0         0.0   \n",
       "3            0.0                0.0          0.0         0.0         0.0   \n",
       "4            0.0                0.0          0.0         0.0         0.0   \n",
       "\n",
       "   Pleurisy  Pneumonia  Resp Failure  Septicemia  Shock  Any Acute  \\\n",
       "0       0.0        0.0           0.0         1.0    1.0        1.0   \n",
       "1       0.0        0.0           0.0         0.0    0.0        1.0   \n",
       "2       0.0        0.0           0.0         0.0    0.0        1.0   \n",
       "3       0.0        0.0           0.0         0.0    0.0        1.0   \n",
       "4       0.0        0.0           0.0         0.0    0.0        0.0   \n",
       "\n",
       "   Any Chronic  train  \n",
       "0          0.0      1  \n",
       "1          1.0      0  \n",
       "2          1.0      0  \n",
       "3          1.0      1  \n",
       "4          1.0      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19392, 42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2101-10-20T19:10:11.000000000' '2175-05-30T21:30:54.000000000'\n",
      " '2149-11-09T13:07:02.000000000' ... '2181-01-29T05:33:34.000000000'\n",
      " '2184-12-28T17:30:58.000000000' '2147-02-08T13:53:58.000000000']\n",
      "19391\n"
     ]
    }
   ],
   "source": [
    "array1 = cohort['intime'].unique()\n",
    "print(array1)\n",
    "print(array1.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>chartdate</th>\n",
       "      <th>charttime</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>22532</td>\n",
       "      <td>167853.0</td>\n",
       "      <td>2151-08-04</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>service: addendum:  radiologic s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>13702</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>2118-06-14</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>date of birth:                   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>26880</td>\n",
       "      <td>135453.0</td>\n",
       "      <td>2162-03-25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>date of birth:           ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>180</td>\n",
       "      <td>20646</td>\n",
       "      <td>134727.0</td>\n",
       "      <td>2112-12-10</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>service: medicine  aller...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>181</td>\n",
       "      <td>42130</td>\n",
       "      <td>114236.0</td>\n",
       "      <td>2150-03-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>date of birth:           ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   note_id  subject_id   hadm_id  chartdate charttime           category  \\\n",
       "0      174       22532  167853.0 2151-08-04       NaT  Discharge summary   \n",
       "1      175       13702  107527.0 2118-06-14       NaT  Discharge summary   \n",
       "4      178       26880  135453.0 2162-03-25       NaT  Discharge summary   \n",
       "6      180       20646  134727.0 2112-12-10       NaT  Discharge summary   \n",
       "7      181       42130  114236.0 2150-03-01       NaT  Discharge summary   \n",
       "\n",
       "                                                text  \n",
       "0                service: addendum:  radiologic s...  \n",
       "1               date of birth:                   ...  \n",
       "4                       date of birth:           ...  \n",
       "6                        service: medicine  aller...  \n",
       "7                       date of birth:           ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(425549, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_id\n",
      "3        20\n",
      "6         8\n",
      "9        23\n",
      "12       23\n",
      "13       10\n",
      "         ..\n",
      "99973    14\n",
      "99982     1\n",
      "99985    64\n",
      "99991    17\n",
      "99995     1\n",
      "Length: 19224, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(notes.groupby('subject_id').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write code for question 2 here\n",
    "# (a)What is the distribution of the cohort (in percentages) for gender, ethnicity, language, and insurance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cohort = pd.DataFrame(cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Distribution of gender--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "M    56.9%\n",
       "F    43.1%\n",
       "Name: gender, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--Distribution of gender--\")\n",
    "series_gender = df_cohort.gender\n",
    "percent_gender = series_gender.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n",
    "percent_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Distribution of ethnicity--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "white       70.1%\n",
       "other       16.9%\n",
       "black        7.6%\n",
       "hispanic     3.2%\n",
       "asian        2.3%\n",
       "Name: ethnicity, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--Distribution of ethnicity--\")\n",
    "series_ethnicity = df_cohort.ethnicity\n",
    "percent_ethnicity = series_ethnicity.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n",
    "percent_ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Distribution of language--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "English    51.2%\n",
       "Missing    40.5%\n",
       "Other       8.3%\n",
       "Name: language, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--Distribution of language--\")\n",
    "series_language = df_cohort.language\n",
    "percent_language = series_language.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n",
    "percent_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Distribution of insurance--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Medicare      54.0%\n",
       "Private       33.7%\n",
       "Medicaid       8.3%\n",
       "Government     2.9%\n",
       "Self Pay       1.1%\n",
       "Name: insurance, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--Distribution of insurance--\")\n",
    "series_insurance = df_cohort.insurance\n",
    "percent_insurance = series_insurance.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n",
    "percent_insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) What is the distribution of the cohort (in percentages) for the intersection of gender and ethnicity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M_white       40.2%\n",
       "F_white       29.9%\n",
       "M_other       10.0%\n",
       "F_other        6.9%\n",
       "F_black        4.2%\n",
       "M_black        3.4%\n",
       "M_hispanic     1.9%\n",
       "M_asian        1.4%\n",
       "F_hispanic     1.3%\n",
       "F_asian        0.9%\n",
       "Name: gender_ethnicity, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cohort['gender_ethnicity'] = df_cohort['gender'] +'_'+ df_cohort['ethnicity']\n",
    "series_gender_ethnicity = df_cohort.gender_ethnicity\n",
    "percent_gender_ethnicity = series_gender_ethnicity.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n",
    "percent_gender_ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       ".tg  {border-collapse:collapse;border-spacing:0;width:100%}\n",
       ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
       ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
       ".tg .tg-fymr{font-weight:bold;border-color:inherit;text-align:left;vertical-align:top}\n",
       ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
       "</style>\n",
       "<table class=\"tg\">\n",
       "  <tr>\n",
       "    <th class=\"tg-fymr\" style=\"width:20%\">Fairness Property</th>\n",
       "    <th class=\"tg-fymr\" style=\"width:40%\">Definition</th>\n",
       "    <th class=\"tg-fymr\" style=\"width:15%\">Gap Name</th>\n",
       "    <th class=\"tg-fymr\" style=\"width:25%\">Gap Equation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"tg-0pky\">Demographic parity</td>\n",
       "    <td class=\"tg-0pky\">$P(\\hat{Y}=y) = P(\\hat{Y} = \\hat{y} | Z=z),&nbsp;&nbsp;\\forall z \\in Z $</td>\n",
       "    <td class=\"tg-0pky\">Parity Gap</td>\n",
       "    <td class=\"tg-0pky\">$\\frac{TP_1 + FP_1}{N_1} - \\frac{TP_2 + FP_2}{N_2}$</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"tg-0pky\">Equality of opportunity<br>(positive class)</td>\n",
       "    <td class=\"tg-0pky\">$P(\\hat{Y}=1 | Y=1) =&nbsp;&nbsp;P(\\hat{Y}=1 | Y=1, Z=z), \\forall z \\in Z$</td>\n",
       "    <td class=\"tg-0pky\">Recall Gap</td>\n",
       "    <td class=\"tg-0pky\">$\\frac{TP_1 }{TP_1+FN_1} - \\frac{TP_2 }{TP_2 + FN_2}$</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"tg-0pky\">Equality of opportunity<br>(negative class)</td>\n",
       "    <td class=\"tg-0pky\">$P(\\hat{Y}=0 | Y=0) =&nbsp;&nbsp;P(\\hat{Y}=0 | Y=0, Z=z), \\forall z \\in Z$</td>\n",
       "    <td class=\"tg-0pky\">Specificity Gap</td>\n",
       "    <td class=\"tg-0pky\">$\\frac{TN_1 }{TN_1+FP_1} - \\frac{TN_2 }{TN_2 + FP_2}$</td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;width:100%}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-fymr{font-weight:bold;border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-fymr\" style=\"width:20%\">Fairness Property</th>\n",
    "    <th class=\"tg-fymr\" style=\"width:40%\">Definition</th>\n",
    "    <th class=\"tg-fymr\" style=\"width:15%\">Gap Name</th>\n",
    "    <th class=\"tg-fymr\" style=\"width:25%\">Gap Equation</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Demographic parity</td>\n",
    "    <td class=\"tg-0pky\">$P(\\hat{Y}=y) = P(\\hat{Y} = \\hat{y} | Z=z),&nbsp;&nbsp;\\forall z \\in Z $</td>\n",
    "    <td class=\"tg-0pky\">Parity Gap</td>\n",
    "    <td class=\"tg-0pky\">$\\frac{TP_1 + FP_1}{N_1} - \\frac{TP_2 + FP_2}{N_2}$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Equality of opportunity<br>(positive class)</td>\n",
    "    <td class=\"tg-0pky\">$P(\\hat{Y}=1 | Y=1) =&nbsp;&nbsp;P(\\hat{Y}=1 | Y=1, Z=z), \\forall z \\in Z$</td>\n",
    "    <td class=\"tg-0pky\">Recall Gap</td>\n",
    "    <td class=\"tg-0pky\">$\\frac{TP_1 }{TP_1+FN_1} - \\frac{TP_2 }{TP_2 + FN_2}$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Equality of opportunity<br>(negative class)</td>\n",
    "    <td class=\"tg-0pky\">$P(\\hat{Y}=0 | Y=0) =&nbsp;&nbsp;P(\\hat{Y}=0 | Y=0, Z=z), \\forall z \\in Z$</td>\n",
    "    <td class=\"tg-0pky\">Specificity Gap</td>\n",
    "    <td class=\"tg-0pky\">$\\frac{TN_1 }{TN_1+FP_1} - \\frac{TN_2 }{TN_2 + FP_2}$</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "import metrics    #metrics_clip.ipynb\n",
    "importlib.reload(metrics)\n",
    "\n",
    "def binary_test():\n",
    "    test_df1 = pd.DataFrame(data  = {\n",
    "        'protected': ['A','B']*5,\n",
    "        'target': [0]*3 + [1]*7,\n",
    "        'pred':[0.1, 0.8, 0.8, 0.9, 0.3, 0.3, 0.99, 0.1, 0.1, 0.9]\n",
    "    })\n",
    "    ret = metrics.gap_metrics(test_df1, 'protected', 'target', 'pred', 0.4)\n",
    "    assert(np.allclose(ret['parity']['A'], -0.2) and\n",
    "    np.allclose(ret['parity']['B'], 0.2) and\n",
    "    np.allclose(ret['specificity']['A'], 0.5) and\n",
    "    np.allclose(ret['specificity']['B'], -0.5) and\n",
    "    np.allclose(ret['recall']['A'], -1/6) and\n",
    "    np.allclose(ret['recall']['B'], 1/6)) , 'Test failed!'\n",
    "    print(\"Test passed!\")\n",
    "    \n",
    "binary_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Model for ICU Mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of df_new: (425549, 49)\n",
      "The shape of df_new: (110595, 7)\n",
      "--------------------df_new-----------------\n",
      "    subject_id  hadm_id_x                                               text  \\\n",
      "1            3     145834  respiratory care note:     patient remains int...   \n",
      "3            3     145834  micu progress nursing note:  patient arrived i...   \n",
      "4            3     145834  micu nursing progress note:  neuro: remains mo...   \n",
      "5            3     145834  npn 7-7p  neuro: pt is alert, opens eyes to ve...   \n",
      "13           3     145834  micu nsg prog note: days remains stable on hig...   \n",
      "14           3     145834  micu npn 3pm-11pm: neuro: pt is restless at ti...   \n",
      "15           3     145834  npn addendum: order for additional ns bolus 50...   \n",
      "16           3     145834  resp. care note pt intubated and vented on set...   \n",
      "21           6     107064  nursing note      vss, afebrile. u/o60-100/ hr...   \n",
      "22           6     107064  nursing note: see careview for complete detail...   \n",
      "\n",
      "   gender  mort_icu  train  diff_days  \n",
      "1       M         0      1   1.963762  \n",
      "3       M         0      1   0.491539  \n",
      "4       M         0      1   1.450567  \n",
      "5       M         0      1   1.946400  \n",
      "13      M         0      1   0.803345  \n",
      "14      M         0      1   1.088762  \n",
      "15      M         0      1   1.145012  \n",
      "16      M         0      1   1.392928  \n",
      "21      F         0      0   1.259097  \n",
      "22      F         0      0   1.843125  \n",
      "subject_id    110595\n",
      "hadm_id_x     110595\n",
      "text          110595\n",
      "gender        110595\n",
      "mort_icu      110595\n",
      "train         110595\n",
      "diff_days     110595\n",
      "dtype: int64\n",
      "--------------------df_text-----------------\n",
      "   subject_id                                        merged_text  \\\n",
      "0           3  respiratory care note:     patient remains int...   \n",
      "1           6  nursing note      vss, afebrile. u/o60-100/ hr...   \n",
      "2           9  nursing note 13:30-7pm admitted from er s/p cv...   \n",
      "3          12  respiratory care:  patient remains intubated o...   \n",
      "4          13  ccu npn s- having cpain left, l shoulder, arm,...   \n",
      "5          17  op day minimally invasive pfo repair nsr. no e...   \n",
      "6          21  admission note 87 yr old man with h/o cad, cva...   \n",
      "7          25  ccu progress note:  this is a 58 yr old male p...   \n",
      "8          26  nsg progress note       pt is a 72 yo man with...   \n",
      "9          31  npn micu a (nsicu pt.) 7p-7a  72 yo male admit...   \n",
      "\n",
      "   merged_text_length  \n",
      "0               10547  \n",
      "1                4260  \n",
      "2                7727  \n",
      "3                4231  \n",
      "4                5581  \n",
      "5                3914  \n",
      "6                7320  \n",
      "7                7273  \n",
      "8                5341  \n",
      "9                6782  \n",
      "subject_id            14851\n",
      "merged_text           14851\n",
      "merged_text_length    14851\n",
      "dtype: int64\n",
      "--------------------df_for_mean-----------------\n",
      "             mean\n",
      "subject_id       \n",
      "3           10547\n",
      "6            4260\n",
      "9            7727\n",
      "12           4231\n",
      "13           5581\n",
      "17           3914\n",
      "21           7320\n",
      "25           7273\n",
      "26           5341\n",
      "31           6782\n",
      "mean    14851\n",
      "dtype: int64\n",
      "--------------------df_new_group-----------------\n",
      "     subject_id  hadm_id_x gender  mort_icu  train  diff_days\n",
      "1             3     145834      M         0      1   1.963762\n",
      "21            6     107064      F         0      0   1.259097\n",
      "31            9     150750      M         1      0   0.198588\n",
      "53           12     112213      M         0      1   1.028970\n",
      "78           13     143045      F         0      1   0.680961\n",
      "85           17     194023      F         0      1   0.283472\n",
      "91           21     109451      M         0      1   0.443704\n",
      "106          25     129635      M         0      0   0.588623\n",
      "116          26     197661      M         0      1   0.304514\n",
      "128          31     128652      M         1      0   1.257153\n",
      "subject_id    14851\n",
      "hadm_id_x     14851\n",
      "gender        14851\n",
      "mort_icu      14851\n",
      "train         14851\n",
      "diff_days     14851\n",
      "dtype: int64\n",
      "The number of patients remaining in the cohort are 14851\n",
      "--------------------df-----------------\n",
      "   subject_id  hadm_id_x gender  mort_icu  train  diff_days  \\\n",
      "0           3     145834      M         0      1   1.963762   \n",
      "1           6     107064      F         0      0   1.259097   \n",
      "2           9     150750      M         1      0   0.198588   \n",
      "3          12     112213      M         0      1   1.028970   \n",
      "4          13     143045      F         0      1   0.680961   \n",
      "5          17     194023      F         0      1   0.283472   \n",
      "6          21     109451      M         0      1   0.443704   \n",
      "7          25     129635      M         0      0   0.588623   \n",
      "8          26     197661      M         0      1   0.304514   \n",
      "9          31     128652      M         1      0   1.257153   \n",
      "\n",
      "                                         merged_text  merged_text_length  \\\n",
      "0  respiratory care note:     patient remains int...               10547   \n",
      "1  nursing note      vss, afebrile. u/o60-100/ hr...                4260   \n",
      "2  nursing note 13:30-7pm admitted from er s/p cv...                7727   \n",
      "3  respiratory care:  patient remains intubated o...                4231   \n",
      "4  ccu npn s- having cpain left, l shoulder, arm,...                5581   \n",
      "5  op day minimally invasive pfo repair nsr. no e...                3914   \n",
      "6  admission note 87 yr old man with h/o cad, cva...                7320   \n",
      "7  ccu progress note:  this is a 58 yr old male p...                7273   \n",
      "8  nsg progress note       pt is a 72 yo man with...                5341   \n",
      "9  npn micu a (nsicu pt.) 7p-7a  72 yo male admit...                6782   \n",
      "\n",
      "    mean  \n",
      "0  10547  \n",
      "1   4260  \n",
      "2   7727  \n",
      "3   4231  \n",
      "4   5581  \n",
      "5   3914  \n",
      "6   7320  \n",
      "7   7273  \n",
      "8   5341  \n",
      "9   6782  \n",
      "subject_id            14851\n",
      "hadm_id_x             14851\n",
      "gender                14851\n",
      "mort_icu              14851\n",
      "train                 14851\n",
      "diff_days             14851\n",
      "merged_text           14851\n",
      "merged_text_length    14851\n",
      "mean                  14851\n",
      "dtype: int64\n",
      "The average length (in characters) of the notes is 8731.668574506768\n"
     ]
    }
   ],
   "source": [
    "# code for 4(a)\n",
    "\n",
    "def process_data(cohort, notes):\n",
    "    '''\n",
    "    Processes data according to the steps in 4a\n",
    "    Inputs :\n",
    "        - cohort dataframe\n",
    "        - notes dataframe\n",
    "    Output:\n",
    "        - Dataframe with (at least) the following columns:\n",
    "            - subject_id, hadm_id\n",
    "            - text\n",
    "            - gender\n",
    "            - mort_icu\n",
    "            - train\n",
    "    '''\n",
    "    # merge two df on the column 'subject_id'\n",
    "    df_new = pd.merge(cohort, notes, on='subject_id')\n",
    "    print(\"The shape of df_new:\", df_new.shape)\n",
    "    \n",
    "    # add a new column named \"diff_days\", which calculated the difference between the two coluumns\n",
    "    # and the unit is in days\n",
    "    df_new['diff_days'] = (df_new['charttime'] - df_new['intime'])/np.timedelta64(1,'D')\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # only keep the days <= 2, i.e., 48h\n",
    "    df_new = df_new[df_new['diff_days'] <= 2]\n",
    "    \n",
    "    # remove 'Discharge summary'\n",
    "    df_new = df_new[df_new['category'] != \"Discharge summar\"]\n",
    "    \n",
    "    #only keep the reuqired columns\n",
    "    df_new = df_new[['subject_id', 'hadm_id_x', 'text', 'gender', 'mort_icu', 'train', 'diff_days']]\n",
    "    print(\"The shape of df_new:\", df_new.shape)\n",
    "    print(\"--------------------df_new-----------------\")\n",
    "    print(df_new.head(10))\n",
    "    print(df_new.count())\n",
    "    \n",
    "    # https://stackoverflow.com/questions/27298178/concatenate-strings-from-several-rows-using-pandas-groupby/45925961\n",
    "    # create a new df, which contanent all text in one cell\n",
    "    df_text = df_new.groupby(['subject_id'])['text'].apply(' '.join).reset_index()\n",
    "    \n",
    "    df_text.rename(columns={'text': 'merged_text'}, inplace=True)\n",
    "    \n",
    "    df_text[\"merged_text_length\"]= df_text[\"merged_text\"].str.len()\n",
    "    print(\"--------------------df_text-----------------\")\n",
    "    print(df_text.head(10))\n",
    "    print(df_text.count())\n",
    "    \n",
    "    \n",
    "    # create a new df: df for calculate mean\n",
    "    # https://stackoverflow.com/questions/32965589/find-min-max-and-average-of-an-id-in-python-pandas\n",
    "    df_for_mean = df_text.groupby('subject_id')['merged_text_length'].agg([pd.np.mean])\n",
    "    print(\"--------------------df_for_mean-----------------\")\n",
    "    print(df_for_mean.head(10))\n",
    "    print(df_for_mean.count())\n",
    "    \n",
    "    \n",
    "    # create a new df: only contains unique subject_id\n",
    "    df_new_group = df_new.drop_duplicates(subset=['subject_id'])\n",
    "    df_new_group = df_new_group[['subject_id', 'hadm_id_x', 'gender', 'mort_icu', 'train', 'diff_days']]\n",
    "    \n",
    "    print(\"--------------------df_new_group-----------------\")\n",
    "    print(df_new_group.head(10))\n",
    "    print(df_new_group.count())\n",
    "    \n",
    "    # merge the new df (df_text) with the original one (df_new)\n",
    "    df = pd.merge(df_new_group, df_text, on='subject_id')\n",
    "    df = pd.merge(df, df_for_mean, on='subject_id')\n",
    "    \n",
    "    # The number of patients remaining in the cohort.\n",
    "    print(\"The number of patients remaining in the cohort are\", df.shape[0])\n",
    "    \n",
    "    # The average length (in characters) of the notes (for those that remain).\n",
    "    # So, this will also count the length of space, digits...\n",
    "    \n",
    "    print(\"--------------------df-----------------\")\n",
    "    print(df.head(10))\n",
    "    print(df.count())\n",
    "    print(\"The average length (in characters) of the notes is\", df[\"merged_text_length\"].mean())\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = process_data(cohort, notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For women:\n",
      "Total number of women is: 6348\n",
      "The average length (in characters) of the notes is (for all women): 8823.981253938247\n",
      "For men:\n",
      "Total number of men is: 8503\n",
      "The average length (in characters) of the notes is (for all men): 8662.751617076327\n"
     ]
    }
   ],
   "source": [
    "# code for 4b part 1/2\n",
    "# code for 4a, please see the above cell\n",
    "\n",
    "# https://www.interviewqs.com/ddi_code_snippets/rows_cols_python\n",
    "df_female = df.loc[df['gender'] == 'F']\n",
    "df_male = df.loc[df['gender'] == 'M']\n",
    "print(\"For women:\")\n",
    "print(\"Total number of women is:\", df_female.gender.count())\n",
    "print(\"The average length (in characters) of the notes is (for all women):\", df_female[\"merged_text_length\"].mean())\n",
    "\n",
    "print(\"For men:\")\n",
    "print(\"Total number of men is:\", df_male.gender.count())\n",
    "print(\"The average length (in characters) of the notes is (for all men):\", df_male[\"merged_text_length\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id_x</th>\n",
       "      <th>gender</th>\n",
       "      <th>mort_icu</th>\n",
       "      <th>train</th>\n",
       "      <th>diff_days</th>\n",
       "      <th>merged_text</th>\n",
       "      <th>merged_text_length</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>107064</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.259097</td>\n",
       "      <td>nursing note      vss, afebrile. u/o60-100/ hr...</td>\n",
       "      <td>4260</td>\n",
       "      <td>4260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>143045</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.680961</td>\n",
       "      <td>ccu npn s- having cpain left, l shoulder, arm,...</td>\n",
       "      <td>5581</td>\n",
       "      <td>5581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>194023</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.283472</td>\n",
       "      <td>op day minimally invasive pfo repair nsr. no e...</td>\n",
       "      <td>3914</td>\n",
       "      <td>3914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35</td>\n",
       "      <td>166707</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.296539</td>\n",
       "      <td>csru update neuro/pain: pt sleepy this am thou...</td>\n",
       "      <td>4727</td>\n",
       "      <td>4727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>59</td>\n",
       "      <td>104130</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.941806</td>\n",
       "      <td>npn 7a-7p dnr please see carevue and fhp for a...</td>\n",
       "      <td>3752</td>\n",
       "      <td>3752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>64</td>\n",
       "      <td>172056</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.757396</td>\n",
       "      <td>micu npn: neuro: a&amp;ox needs much encouragement...</td>\n",
       "      <td>5673</td>\n",
       "      <td>5673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>68</td>\n",
       "      <td>170467</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.595648</td>\n",
       "      <td>resp care  sputum induction obtained for pcp a...</td>\n",
       "      <td>4602</td>\n",
       "      <td>4602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>71</td>\n",
       "      <td>111944</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.878600</td>\n",
       "      <td>nursing note (0700-1900hrs)  ms.  has slowly b...</td>\n",
       "      <td>5832</td>\n",
       "      <td>5832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>73</td>\n",
       "      <td>194730</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.188808</td>\n",
       "      <td>ccu npn 7p-7a 57 y.o r/i mi s/p vfib arrest @ ...</td>\n",
       "      <td>2981</td>\n",
       "      <td>2981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>83</td>\n",
       "      <td>158569</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339907</td>\n",
       "      <td>ccu nursing progress note pls see fhpa/admit n...</td>\n",
       "      <td>4413</td>\n",
       "      <td>4413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject_id  hadm_id_x gender  mort_icu  train  diff_days  \\\n",
       "1            6     107064      F         0      0   1.259097   \n",
       "4           13     143045      F         0      1   0.680961   \n",
       "5           17     194023      F         0      1   0.283472   \n",
       "11          35     166707      F         0      0   1.296539   \n",
       "16          59     104130      F         0      1   1.941806   \n",
       "19          64     172056      F         0      1   1.757396   \n",
       "21          68     170467      F         0      1   0.595648   \n",
       "22          71     111944      F         0      1   1.878600   \n",
       "23          73     194730      F         0      1   0.188808   \n",
       "25          83     158569      F         0      0   0.339907   \n",
       "\n",
       "                                          merged_text  merged_text_length  \\\n",
       "1   nursing note      vss, afebrile. u/o60-100/ hr...                4260   \n",
       "4   ccu npn s- having cpain left, l shoulder, arm,...                5581   \n",
       "5   op day minimally invasive pfo repair nsr. no e...                3914   \n",
       "11  csru update neuro/pain: pt sleepy this am thou...                4727   \n",
       "16  npn 7a-7p dnr please see carevue and fhp for a...                3752   \n",
       "19  micu npn: neuro: a&ox needs much encouragement...                5673   \n",
       "21  resp care  sputum induction obtained for pcp a...                4602   \n",
       "22  nursing note (0700-1900hrs)  ms.  has slowly b...                5832   \n",
       "23  ccu npn 7p-7a 57 y.o r/i mi s/p vfib arrest @ ...                2981   \n",
       "25  ccu nursing progress note pls see fhpa/admit n...                4413   \n",
       "\n",
       "    mean  \n",
       "1   4260  \n",
       "4   5581  \n",
       "5   3914  \n",
       "11  4727  \n",
       "16  3752  \n",
       "19  5673  \n",
       "21  4602  \n",
       "22  5832  \n",
       "23  2981  \n",
       "25  4413  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_female.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.123365968449534, pvalue=0.2613002584853309)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "ttest_ind(df_female['mean'], df_male['mean'])\n",
    "\n",
    "# to judge whether there is a significant difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of women is: 6348\n",
      "Total number of mortality in ICU for women is: 633\n",
      "The prevalence of mort icu for women is: 0.0997164461247637\n",
      "i.e., 9.971644612476371%\n",
      "---------------------------------------\n",
      "Total number of men is: 8503\n",
      "Total number of mortality in ICU for men is: 772\n",
      "The prevalence of mort icu for men is: 0.09079148535810891\n",
      "i.e., 9.07914853581089%\n"
     ]
    }
   ],
   "source": [
    "# code for 4c\n",
    "# Report the prevalence of mort icu for men versus women \n",
    "# sum(mort_ice) of men or women/ total num of men or women\n",
    "print(\"Total number of women is:\", df_female.gender.count())\n",
    "print(\"Total number of mortality in ICU for women is:\", df_female.mort_icu.sum())\n",
    "print(\"The prevalence of mort icu for women is:\", df_female.mort_icu.sum()/ df_female.gender.count())\n",
    "print(\"i.e.,\", (df_female.mort_icu.sum() * 100/ df_female.gender.count()).astype(str) + '%')\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Total number of men is:\", df_male.gender.count())\n",
    "print(\"Total number of mortality in ICU for men is:\", df_male.mort_icu.sum())\n",
    "print(\"The prevalence of mort icu for men is:\", df_male.mort_icu.sum()/ df_male.gender.count())\n",
    "print(\"i.e.,\", (df_male.mort_icu.sum() * 100/ df_male.gender.count()).astype(str) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for 4d\n",
    "# Your model should achieve at least 85% test AUROC. Report your model AUROC and AUPRC on the test set.\n",
    "# now, manipilate df\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [respiratory, care, note, patient, remains, in...\n",
       "1        [nursing, note, vss, afebrile, u/o60-100/, hr,...\n",
       "2        [nursing, note, 13:30-7pm, admitted, er, s/p, ...\n",
       "3        [respiratory, care, patient, remains, intubate...\n",
       "4        [ccu, npn, s-, cpain, left, l, shoulder, arm, ...\n",
       "                               ...                        \n",
       "14846    [admitted, osh, fall, home, .after, using, br,...\n",
       "14847    [year, old, male, admitted, hospital, severe, ...\n",
       "14848    [yr, woman, nka, admitted, ed, early, morning,...\n",
       "14849    [year-old, male, history, hodkin, 's, s/p, bmt...\n",
       "14850    [pt, 47yr, old, male, recurrent, diverticuliti...\n",
       "Name: merged_text, Length: 14851, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reuse code from HW1 4b\n",
    "#use NLTK to tokenize and remove stopwords and punctuation\n",
    "stoplist = stopwords.words('english') + list(string.punctuation)\n",
    "df['merged_text'] = df['merged_text'].astype(str)\n",
    "stoplist = set(stoplist)\n",
    "def remove_punction_and_stopwords(text):\n",
    "    return [word for word in word_tokenize(text) if word.lower() not in stoplist and not word.isdigit()]\n",
    "df['merged_text'].apply(remove_punction_and_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4d\n",
    "#use TF-IDF to transform bag-of-word counts into numerical features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# seperate training set and testing set\n",
    "train_df = df[df.train == 1] # you may want to create a validation set from this\n",
    "test_df = df[df.train == 0] \n",
    "\n",
    "# for code to be readable, rename train_df and test_df to X_train and X_test, respectively\n",
    "X_train = train_df\n",
    "X_test = test_df\n",
    "# drop_cols = ['train', 'subject_id', 'hadm_id_x', 'diff_days', 'mort_icu', 'merged_text_length', 'gender']\n",
    "y_train = train_df['mort_icu']\n",
    "y_test = test_df['mort_icu']\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train['merged_text'])\n",
    "X_test = vectorizer.transform(X_test['merged_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your model training and evaluation code here\n",
    "\n",
    "# traing: \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty = 'l2', max_iter=100)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of y_predict:\n",
      "[0 0 0 ... 0 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "(4397,)\n",
      "The result of y_predict_prob:\n",
      "[[0.98607492 0.01392508]\n",
      " [0.66520667 0.33479333]\n",
      " [0.96147103 0.03852897]\n",
      " ...\n",
      " [0.96454938 0.03545062]\n",
      " [0.91277044 0.08722956]\n",
      " [0.98244352 0.01755648]]\n",
      "<class 'numpy.ndarray'>\n",
      "(4397, 2)\n",
      "Score on training set:  0.9179261526688349\n",
      "Score on testing set:  0.9194905617466455\n",
      "Accuracy score: 0.9194905617466455\n",
      "No. of iterations to converge:  [53]\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "\n",
    "from sklearn import metrics\n",
    "y_predict = clf.predict(X_test)\n",
    "y_predict_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(\"The result of y_predict:\")\n",
    "print(y_predict)\n",
    "print(type(y_predict))\n",
    "print(y_predict.shape)\n",
    "\n",
    "print(\"The result of y_predict_prob:\")\n",
    "print(y_predict_prob)\n",
    "print(type(y_predict_prob))\n",
    "print(y_predict_prob.shape)\n",
    "\n",
    "# comment on model performance\n",
    "print(\"Score on training set: \", clf.score(X_train,y_train))\n",
    "print(\"Score on testing set: \", clf.score(X_test,y_test))\n",
    "print(\"Accuracy score:\",metrics.accuracy_score(y_test, y_predict))\n",
    "print(\"No. of iterations to converge: \", clf.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_pred = []\n",
    "i = 0\n",
    "for i in range (y_predict_prob.shape[0]):\n",
    "    ls_pred.append(y_predict_prob[i][1])\n",
    "# add probability to test_df df\n",
    "test_df = test_df.assign(ls_pred=ls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under PR Curve(i.e., AUPRC): 0.5094244560801303\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import metrics\n",
    "from sklearn.metrics import *\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_predict_prob[:,1], pos_label = 1)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
    "area = auc(recall, precision)\n",
    "print (\"Area Under PR Curve(i.e., AUPRC):\", area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_roc(fpr, tpr, filename):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='red', label='ROC Graph')\n",
    "    plt.plot([0, 1], [0, 1], color='blue')\n",
    "    plt.xlabel('fpr (False Positive Rate)')\n",
    "    plt.ylabel('tpr (True Positive Rate)')\n",
    "    plt.title('ROC Graph')\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score (i.e., AUROC):  0.8905556059095441\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(fpr, tpr,\"s1_Q4(d).png\")\n",
    "# print(\"The threshod is: \", thresholds)\n",
    "print(\"AUC score (i.e., AUROC): \", metrics.roc_auc_score(y_test, y_predict_prob[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function in your solution for 4e, stratify by gender\n",
    "def stratified_sample(df, column):\n",
    "    '''\n",
    "    Returns a stratified sample of df with replacement (stratified by column), with the same number of rows as df.\n",
    "    '''\n",
    "    grp = df.groupby(column, group_keys = False)\n",
    "    return grp.apply(lambda x: x.sample(n = int(np.rint(len(x))), replace = True)).sample(frac=1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Write code for 4e here\n",
    "# using 1,000 bootstrapped samples of the test set, with gender as the protected variable. Use a decision threshold of 0.3.\n",
    "# i.e. run [stratified_sample + mertics] for 1000 times\n",
    "import metrics\n",
    "importlib.reload(metrics)\n",
    "\n",
    "ls_of_dict = []\n",
    "\n",
    "ls_parity_M = []\n",
    "ls_parity_F = []\n",
    "ls_specificity_M = []\n",
    "ls_specificity_F = []\n",
    "ls_recall_M = []\n",
    "ls_recall_F = []\n",
    "\n",
    "test_df = df[df.train == 0] \n",
    "\n",
    "for i in range(1000):    # 1,000 bootstrapped samples\n",
    "    \n",
    "    #each time stratified_sample will return different order of patients, but still the same size of df\n",
    "    test_df_stratified = stratified_sample(test_df, 'gender')\n",
    "    \n",
    "    # we need the predict probability for df_stratified \n",
    "    y_test_stra = test_df_stratified['mort_icu']\n",
    "    X_test_stra = vectorizer.transform(test_df_stratified['merged_text'])\n",
    "    \n",
    "    y_predict_stra = clf.predict(X_test_stra)\n",
    "    y_predict_prob_stra = clf.predict_proba(X_test_stra)\n",
    "    \n",
    "    ls_pred_stra = []\n",
    "    for i in range (y_predict_prob_stra.shape[0]):\n",
    "        ls_pred_stra.append(y_predict_prob_stra[i][1])\n",
    "\n",
    "        \n",
    "    # add probability to test_df\n",
    "#     test_df_stratified = test_df_stratified.assign(ls_pred_stra = ls_pred_stra)\n",
    "    test_df_stratified['ls_pred_stra'] = pd.Series(ls_pred_stra)\n",
    "    \n",
    "    ls_of_dict.append(metrics.gap_metrics(test_df_stratified, 'gender', 'mort_icu', 'ls_pred_stra', 0.3))\n",
    "    \n",
    "for i in range(len(ls_of_dict)):\n",
    "    ls_parity_M.append(ls_of_dict[i]['parity']['M'])\n",
    "    ls_parity_F.append(ls_of_dict[i]['parity']['F'])\n",
    "    \n",
    "    ls_specificity_M.append(ls_of_dict[i]['specificity']['M'])\n",
    "    ls_specificity_F.append(ls_of_dict[i]['specificity']['F'])\n",
    "    \n",
    "    ls_recall_M.append(ls_of_dict[i]['recall']['M'])\n",
    "    ls_recall_F.append(ls_of_dict[i]['recall']['F'])\n",
    "    \n",
    "print(len(ls_of_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean of parity_M': -0.0022550272182548805, 'mean of parity_F': 0.0022550272182548805, 'mean of specificity_M': 0.00489765562416902, 'mean of specificity_F': -0.00489765562416902, 'mean of recall_M': 0.029900987977574473, 'mean of recall_F': -0.029900987977574473}\n"
     ]
    }
   ],
   "source": [
    "stra_dict = {\"mean of parity_M\" : sum(ls_parity_M) / 1000, \n",
    "            \"mean of parity_F\" : sum(ls_parity_F) / 1000,\n",
    "            \"mean of specificity_M\" : sum(ls_specificity_M) / 1000,\n",
    "            \"mean of specificity_F\" : sum(ls_specificity_F) / 1000,\n",
    "            \"mean of recall_M\" : sum(ls_recall_M) / 1000,\n",
    "            \"mean of recall_F\" : sum(ls_recall_F) / 1000}\n",
    "print(stra_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation of ls_parity_M is 0.007533554092300888 \n",
      "Standard Deviation of ls_parity_F is 0.007533554092300888 \n",
      "Standard Deviation of ls_specificity_M is 0.005735528599101285 \n",
      "Standard Deviation of ls_specificity_F is 0.005735528599101285 \n",
      "Standard Deviation of ls_recall_M is 0.05018580464716909 \n",
      "Standard Deviation of ls_recall_F is 0.05018580464716909 \n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "  \n",
    "# Prints standard deviation  \n",
    "print(\"Standard Deviation of ls_parity_M is % s \" \n",
    "                % (statistics.stdev(ls_parity_M))) \n",
    "print(\"Standard Deviation of ls_parity_F is % s \" \n",
    "                % (statistics.stdev(ls_parity_F)))\n",
    "print(\"Standard Deviation of ls_specificity_M is % s \" \n",
    "                % (statistics.stdev(ls_specificity_M)))\n",
    "print(\"Standard Deviation of ls_specificity_F is % s \" \n",
    "                % (statistics.stdev(ls_specificity_F)))\n",
    "print(\"Standard Deviation of ls_recall_M is % s \" \n",
    "                % (statistics.stdev(ls_recall_M)))\n",
    "print(\"Standard Deviation of ls_recall_F is % s \" \n",
    "                % (statistics.stdev(ls_recall_F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For recall gap, t_val, p_val:  26.64524049472698 4.222312706924797e-134\n",
      "For parity gap, t_val, p_val:  -13.386494845197644 3.3596400529847585e-39\n",
      "For specificity gap, t_val, p_val:  38.18825315505221 4.522522407492755e-240\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats \n",
    "\n",
    "t_val, p_val = stats.ttest_ind(ls_recall_M, ls_recall_F)\n",
    "print(\"For recall gap, t_val, p_val: \", t_val, p_val)\n",
    "\n",
    "t_val, p_val = stats.ttest_ind(ls_parity_M, ls_parity_F)\n",
    "print(\"For parity gap, t_val, p_val: \", t_val, p_val)\n",
    "\n",
    "t_val, p_val = stats.ttest_ind(ls_specificity_M, ls_specificity_F)\n",
    "print(\"For specificity gap, t_val, p_val: \", t_val, p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
